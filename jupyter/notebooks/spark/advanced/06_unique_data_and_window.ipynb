{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9913130b-9ed3-4a87-a01d-a13cae0e3204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Unique data & Window Functions\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6357c0-ccf8-43de-8f7d-69d5a9314fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emp Data & Schema\n",
    "\n",
    "emp_data = [\n",
    "    [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"],\n",
    "    [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"],\n",
    "    [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"],\n",
    "    [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"],\n",
    "    [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"],\n",
    "    [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"],\n",
    "    [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"],\n",
    "    [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"],\n",
    "    [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"],\n",
    "    [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"],\n",
    "    [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"],\n",
    "    [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"],\n",
    "    [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"],\n",
    "    [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"],\n",
    "    [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"],\n",
    "    [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"],\n",
    "    [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"],\n",
    "    [\"018\",\"104\",\"Nancy Liu\",\"29\",\"\",\"50000\",\"2017-06-01\"],\n",
    "    [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"],\n",
    "    [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"]\n",
    "]\n",
    "\n",
    "emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc19a5-b213-498e-8e1f-63650523c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create emp DataFrame\n",
    "\n",
    "emp = spark.createDataFrame(data=emp_data, schema=emp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81001d-2633-439f-9153-9b1ef27e64a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show emp dataframe (ACTION)\n",
    "\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340333a-1ff1-4b60-a602-6342a056cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Schema\n",
    "\n",
    "emp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99ead2-9509-43b0-83e9-a8f1eaebffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique data\n",
    "# select distinct emp.* from emp\n",
    "emp_unique = emp.distinct()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc51ab-910b-41ff-bc29-10f9fc0a4181",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_unique.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42b4ccb-373b-404c-aa89-db6a40bf3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique of department_ids\n",
    "# select distinct department_id from emp\n",
    "emp_dept_id = emp.select(\"department_id\").distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eefbe8f-b34d-4e25-8c3c-0a3bd26b2748",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_dept_id.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e8d717-c449-42f4-a3e3-6bff56077b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window Functions\n",
    "# select *, max(salary) over(partition by department_id order by salary desc) as max_salary from emp_unique\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import max, col, desc\n",
    "\n",
    "window_spec = Window.partitionBy(col(\"department_id\")).orderBy(col(\"salary\").desc())\n",
    "max_func = max(col(\"salary\")).over(window_spec)\n",
    "\n",
    "emp_1 = emp.withColumn(\"max_salary\", max_func)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc42d609-f44f-43eb-a63e-f98c23fd8204",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_1.show()\n",
    "\n",
    "# Window Functions - 2nd highest salary of each department\n",
    "# select *, row_number() over(partition by department_id order by salary desc) as rn from emp_unique where rn = 2\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc, col\n",
    "\n",
    "window_spec = Window.partitionBy(col(\"department_id\")).orderBy(col(\"salary\").desc())\n",
    "rn = row_number().over(window_spec)\n",
    "\n",
    "emp_2 = emp.withColumn(\"rn\", rn).where(\"rn = 2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc6295-4e8f-44a4-8baf-e45f1739248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a249be-bfab-4bde-ae61-eb6782660b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window function using expr\n",
    "# select *, row_number() over(partition by department_id order by salary desc) as rn from emp_unique where rn = 2\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "emp_3 = emp.withColumn(\"rn\", expr(\"row_number() over(partition by department_id order by salary desc)\")).where(\"rn = 2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590c9bb-3519-4511-921b-e851f385deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7ba53-5875-4e9c-8e95-a29b0b3e73b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus TIP\n",
    "# Databricks community cloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
