{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34889865-2884-4661-bbea-b71d9c28af13",
   "metadata": {},
   "source": [
    "## Spark Streaming read from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383273d0-cdd6-4027-b010-eac40bd3f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming from Kafka\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419debac-31b7-4fe8-bca2-4b858836e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the streaming_df to read from kafka\n",
    "streaming_df = spark.readStream\\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"devices\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284ebf2-3269-46e9-b361-c0932884ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To the schema of the data,  post a kafka message and change readStream to read \n",
    "# streaming_df.printSchema()\n",
    "# streaming_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20666f8-0ccf-40a5-bc33-1b1b35284687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Schema\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType\n",
    "json_schema = StructType([StructField('customerId', StringType(), True), \\\n",
    "StructField('data', StructType([StructField('devices', ArrayType(StructType([ \\\n",
    "StructField('deviceId', StringType(), True), \\\n",
    "StructField('measure', StringType(), True), \\\n",
    "StructField('status', StringType(), True), \\\n",
    "StructField('temperature', LongType(), True)]), True), True)]), True), \\\n",
    "StructField('eventId', StringType(), True), \\\n",
    "StructField('eventOffset', LongType(), True), \\\n",
    "StructField('eventPublisher', StringType(), True), \\\n",
    "StructField('eventTime', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b51ba5-b247-4d49-9cd6-ca9753e132d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse value from binay to string\n",
    "json_df = streaming_df.selectExpr(\"cast(value as string) as value\")\n",
    "\n",
    "# Apply Schema to JSON value column and expand the value\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "json_expanded_df = json_df.withColumn(\"value\", from_json(json_df[\"value\"], json_schema)).select(\"value.*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c9902-bf6a-4fee-be80-7b8732382556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Schema\n",
    "# json_expanded_df.show(10, False)\n",
    "# json_expanded_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f879dd5-62af-42b7-8201-c4c5053e5b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets explode the data as devices contains list/array of device reading\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "exploded_df = json_expanded_df \\\n",
    "    .select(\"customerId\", \"eventId\", \"eventOffset\", \"eventPublisher\", \"eventTime\", \"data\") \\\n",
    "    .withColumn(\"devices\", explode(\"data.devices\")) \\\n",
    "    .drop(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f3ec9-1512-4118-9f6b-3dd5443be234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema of the exploded_df,  post a kafka message and change readStream to read \n",
    "# exploded_df.printSchema()\n",
    "# exploded_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027929ff-f5d7-4408-9973-998be7128df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the exploded df\n",
    "flattened_df = exploded_df \\\n",
    "    .selectExpr(\"customerId\", \"eventId\", \"eventOffset\", \"eventPublisher\", \"cast(eventTime as timestamp) as eventTime\", \n",
    "                \"devices.deviceId as deviceId\", \"devices.measure as measure\", \n",
    "                \"devices.status as status\", \"devices.temperature as temperature\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01a060-afa7-4ee1-b1e8-b3a9515969bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema of the flattened_df,  post a kafka message and change readStream to read \n",
    "# flattened_df.printSchema()\n",
    "# flattened_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5461f1-a106-4e33-8d16-6c51b8718161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the dataframes to find the average temparature\n",
    "# per Customer per device throughout the day for SUCCESS events\n",
    "from pyspark.sql.functions import to_date, avg\n",
    "\n",
    "agg_df = flattened_df.where(\"STATUS = 'SUCCESS'\") \\\n",
    "    .withColumn(\"eventDate\", to_date(\"eventTime\", \"yyyy-MM-dd\")) \\\n",
    "    .groupBy(\"customerId\",\"deviceId\",\"eventDate\") \\\n",
    "    .agg(avg(\"temperature\").alias(\"avg_temp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3590f85-6881-4271-bef4-915d00a9bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema of the agg_df, post a kafka message and change readStream to read \n",
    "# agg_df.printSchema()\n",
    "# agg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0057dc-9034-4667-89e6-b62b8028c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output to console sink to check the output\n",
    "writing_df = agg_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"checkpointLocation\",\"checkpoint_dir\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n",
    "    \n",
    "# Start the streaming application to run until the following happens\n",
    "# 1. Exception in the running program\n",
    "# 2. Manual Interruption\n",
    "writing_df.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d71a4-c35f-4e00-9a48-4e7c76f5bbdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
